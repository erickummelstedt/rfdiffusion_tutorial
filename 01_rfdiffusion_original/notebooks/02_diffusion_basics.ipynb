{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef67c5c5",
   "metadata": {},
   "source": [
    "# Module 1: Diffusion Models - Theory and Practice\n",
    "\n",
    "**üìç Notebook 2 of 8**\n",
    "\n",
    "## üíª GPU Requirements\n",
    "**‚úÖ No GPU needed!** All examples run on CPU.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. Understand what diffusion models are and how they work\n",
    "2. Grasp the forward diffusion process (adding noise)\n",
    "3. Grasp the reverse diffusion process (denoising)\n",
    "4. Implement a simple 1D diffusion model from scratch\n",
    "5. Visualize the diffusion process in 2D\n",
    "6. Understand how this applies to protein design\n",
    "\n",
    "## üìö Prerequisites\n",
    "\n",
    "- Basic probability (Gaussian distributions)\n",
    "- Python and NumPy\n",
    "- Basic understanding of neural networks (optional for theory)\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î What Are Diffusion Models?\n",
    "\n",
    "**Core Idea**: Learn to generate data by gradually removing noise.\n",
    "\n",
    "### Analogy: The Sculptor's Approach\n",
    "- Traditional GANs: Sculpt a statue from nothing in one shot ‚ö°\n",
    "- Diffusion Models: Gradually chip away at a block of marble, step by step üî®\n",
    "\n",
    "### Two Processes:\n",
    "\n",
    "**Forward Process** (Training time):\n",
    "```\n",
    "Real Data ‚Üí Add Noise ‚Üí Add More Noise ‚Üí ... ‚Üí Pure Noise\n",
    "  x‚ÇÄ      ‚Üí    x‚ÇÅ     ‚Üí      x‚ÇÇ        ‚Üí ... ‚Üí    x‚Çú\n",
    "```\n",
    "\n",
    "**Reverse Process** (Generation time):\n",
    "```\n",
    "Pure Noise ‚Üí Denoise ‚Üí Denoise More ‚Üí ... ‚Üí Generated Data\n",
    "    x‚Çú     ‚Üí   x‚Çú‚Çã‚ÇÅ  ‚Üí      x‚Çú‚Çã‚ÇÇ     ‚Üí ... ‚Üí      x‚ÇÄ\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1d9d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfacccf",
   "metadata": {},
   "source": [
    "## üé≤ The Forward Diffusion Process\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Starting with data `x‚ÇÄ`, we add Gaussian noise over `T` timesteps:\n",
    "\n",
    "$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I)$$\n",
    "\n",
    "Where:\n",
    "- $\\beta_t$ is the noise schedule (how much noise to add at step $t$)\n",
    "- As $t$ increases, $\\beta_t$ typically increases\n",
    "- At $t=T$, data becomes pure noise: $x_T \\sim \\mathcal{N}(0, I)$\n",
    "\n",
    "### Key Insight: Closed Form\n",
    "\n",
    "We can jump directly to any timestep without intermediate steps:\n",
    "\n",
    "$$q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha}_t) I)$$\n",
    "\n",
    "Where $\\bar{\\alpha}_t = \\prod_{i=1}^t (1-\\beta_i)$\n",
    "\n",
    "**This is crucial for efficient training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd023516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement forward diffusion in 1D\n",
    "def forward_diffusion_1d(x0, timesteps=100):\n",
    "    \"\"\"\n",
    "    Apply forward diffusion to 1D data.\n",
    "    \n",
    "    Args:\n",
    "        x0: Original data point (scalar)\n",
    "        timesteps: Number of diffusion steps\n",
    "    \n",
    "    Returns:\n",
    "        x_trajectory: Array of noisy versions at each timestep\n",
    "        alphas_cumprod: Cumulative product of (1-beta)\n",
    "    \"\"\"\n",
    "    # Define noise schedule (linear)\n",
    "    betas = np.linspace(0.0001, 0.02, timesteps)\n",
    "    alphas = 1 - betas\n",
    "    alphas_cumprod = np.cumprod(alphas)\n",
    "    \n",
    "    # Store trajectory\n",
    "    x_trajectory = np.zeros(timesteps + 1)\n",
    "    x_trajectory[0] = x0\n",
    "    \n",
    "    # Apply diffusion at each step\n",
    "    for t in range(1, timesteps + 1):\n",
    "        # Sample noise\n",
    "        noise = np.random.randn()\n",
    "        \n",
    "        # Apply noise using closed form\n",
    "        sqrt_alpha_cumprod = np.sqrt(alphas_cumprod[t-1])\n",
    "        sqrt_one_minus_alpha_cumprod = np.sqrt(1 - alphas_cumprod[t-1])\n",
    "        \n",
    "        x_trajectory[t] = sqrt_alpha_cumprod * x0 + sqrt_one_minus_alpha_cumprod * noise\n",
    "    \n",
    "    return x_trajectory, alphas_cumprod\n",
    "\n",
    "# Test with a simple value\n",
    "x0 = 5.0  # Original data point\n",
    "x_traj, alphas = forward_diffusion_1d(x0, timesteps=100)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(x_traj, linewidth=2)\n",
    "plt.axhline(y=x0, color='r', linestyle='--', label=f'Original value: {x0}')\n",
    "plt.axhline(y=0, color='g', linestyle='--', alpha=0.5, label='Pure noise mean: 0')\n",
    "plt.xlabel('Timestep', fontsize=12)\n",
    "plt.ylabel('Value', fontsize=12)\n",
    "plt.title('Forward Diffusion: Gradually Adding Noise', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Starting value: {x0}\")\n",
    "print(f\"After 25 steps: {x_traj[25]:.3f}\")\n",
    "print(f\"After 50 steps: {x_traj[50]:.3f}\")\n",
    "print(f\"After 100 steps: {x_traj[100]:.3f} (approaching pure noise)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1238c3",
   "metadata": {},
   "source": [
    "## üîô The Reverse Diffusion Process\n",
    "\n",
    "### The Goal\n",
    "\n",
    "Learn to predict $x_{t-1}$ from $x_t$ (remove noise step by step).\n",
    "\n",
    "### Reverse Distribution\n",
    "\n",
    "$$p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$$\n",
    "\n",
    "Where $\\mu_\\theta$ is a neural network that predicts the mean.\n",
    "\n",
    "### Training Objective\n",
    "\n",
    "Train the network to predict the **noise** that was added:\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{t, x_0, \\epsilon} [\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2]$$\n",
    "\n",
    "Where:\n",
    "- $\\epsilon$ is the actual noise added\n",
    "- $\\epsilon_\\theta$ is the network's prediction\n",
    "- $x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon$\n",
    "\n",
    "**Once trained, we can sample by starting from noise and iteratively denoising!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f392dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple denoising function (oracle - knows the true data)\n",
    "# In practice, this would be a trained neural network\n",
    "def denoise_step(xt, t, x0_true, alphas_cumprod):\n",
    "    \"\"\"\n",
    "    Single reverse diffusion step (oracle version).\n",
    "    \n",
    "    In real diffusion models, we'd use a neural network to predict\n",
    "    the noise instead of using the true x0.\n",
    "    \"\"\"\n",
    "    alpha_cumprod_t = alphas_cumprod[t]\n",
    "    alpha_cumprod_prev = alphas_cumprod[t-1] if t > 0 else 1.0\n",
    "    \n",
    "    # Predict x0 from xt (in practice, network predicts noise)\n",
    "    # Here we cheat and use the true x0 for demonstration\n",
    "    predicted_x0 = x0_true\n",
    "    \n",
    "    # Compute mean of p(x_{t-1} | x_t, x_0)\n",
    "    coef1 = np.sqrt(alpha_cumprod_prev) * (1 - alpha_cumprod_t / alpha_cumprod_prev)\n",
    "    coef2 = np.sqrt(alpha_cumprod_t) * (1 - alpha_cumprod_prev)\n",
    "    mean = (coef1 * predicted_x0 + coef2 * xt) / (1 - alpha_cumprod_t)\n",
    "    \n",
    "    # Add small noise (except at t=0)\n",
    "    if t > 0:\n",
    "        variance = (1 - alpha_cumprod_prev) / (1 - alpha_cumprod_t) * (1 - alpha_cumprod_t / alpha_cumprod_prev)\n",
    "        noise = np.random.randn() * np.sqrt(variance)\n",
    "        return mean + noise\n",
    "    else:\n",
    "        return mean\n",
    "\n",
    "# Perform reverse diffusion\n",
    "x_reverse = np.zeros(101)\n",
    "x_reverse[-1] = x_traj[-1]  # Start from noisy version\n",
    "\n",
    "for t in range(99, -1, -1):\n",
    "    x_reverse[t] = denoise_step(x_reverse[t+1], t, x0, alphas)\n",
    "\n",
    "# Visualize forward and reverse\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Forward process\n",
    "ax1.plot(x_traj, linewidth=2, color='#E63946')\n",
    "ax1.axhline(y=x0, color='black', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Timestep', fontsize=11)\n",
    "ax1.set_ylabel('Value', fontsize=11)\n",
    "ax1.set_title('Forward: Data ‚Üí Noise', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Reverse process\n",
    "ax2.plot(x_reverse, linewidth=2, color='#457B9D')\n",
    "ax2.axhline(y=x0, color='black', linestyle='--', alpha=0.5, label=f'Target: {x0}')\n",
    "ax2.set_xlabel('Timestep', fontsize=11)\n",
    "ax2.set_ylabel('Value', fontsize=11)\n",
    "ax2.set_title('Reverse: Noise ‚Üí Data', fontsize=13, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original value: {x0}\")\n",
    "print(f\"Recovered value: {x_reverse[0]:.3f}\")\n",
    "print(f\"Error: {abs(x_reverse[0] - x0):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb213797",
   "metadata": {},
   "source": [
    "## üé® 2D Example: Swiss Roll Dataset\n",
    "\n",
    "Let's see diffusion in action on 2D data (more visual!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Swiss Roll data\n",
    "def make_swiss_roll(n_samples=1000):\n",
    "    \"\"\"Generate 2D Swiss Roll dataset.\"\"\"\n",
    "    t = 3 * np.pi * (1 + 2 * np.random.rand(n_samples))\n",
    "    x = t * np.cos(t)\n",
    "    y = t * np.sin(t)\n",
    "    X = np.stack([x, y], axis=1)\n",
    "    return X / 10.0  # Scale down\n",
    "\n",
    "# Forward diffusion for 2D data\n",
    "def forward_diffusion_2d(X, t, betas):\n",
    "    \"\"\"Apply forward diffusion to 2D data.\"\"\"\n",
    "    alphas = 1 - betas\n",
    "    alphas_cumprod = np.cumprod(alphas)\n",
    "    \n",
    "    sqrt_alpha_cumprod = np.sqrt(alphas_cumprod[t])\n",
    "    sqrt_one_minus_alpha_cumprod = np.sqrt(1 - alphas_cumprod[t])\n",
    "    \n",
    "    noise = np.random.randn(*X.shape)\n",
    "    X_noisy = sqrt_alpha_cumprod * X + sqrt_one_minus_alpha_cumprod * noise\n",
    "    \n",
    "    return X_noisy\n",
    "\n",
    "# Generate data\n",
    "X_original = make_swiss_roll(500)\n",
    "\n",
    "# Define noise schedule\n",
    "T = 100\n",
    "betas = np.linspace(0.0001, 0.02, T)\n",
    "\n",
    "# Apply diffusion at different timesteps\n",
    "timesteps = [0, 10, 25, 50, 100]\n",
    "fig, axes = plt.subplots(1, len(timesteps), figsize=(18, 3))\n",
    "\n",
    "for idx, t in enumerate(timesteps):\n",
    "    if t == 0:\n",
    "        X_t = X_original\n",
    "        title = \"Original Data (t=0)\"\n",
    "    else:\n",
    "        X_t = forward_diffusion_2d(X_original, t-1, betas)\n",
    "        title = f\"t={t}\"\n",
    "    \n",
    "    axes[idx].scatter(X_t[:, 0], X_t[:, 1], s=2, alpha=0.6, c=range(len(X_t)), cmap='viridis')\n",
    "    axes[idx].set_title(title, fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlim(-4, 4)\n",
    "    axes[idx].set_ylim(-4, 4)\n",
    "    axes[idx].set_aspect('equal')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Forward Diffusion: Structure ‚Üí Noise', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how the structured Swiss Roll gradually becomes random noise!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291810b6",
   "metadata": {},
   "source": [
    "## üß¨ Connection to Protein Design\n",
    "\n",
    "Now let's connect this to RFDiffusion:\n",
    "\n",
    "### What Gets Diffused?\n",
    "Instead of 2D points, we diffuse **3D protein backbone coordinates**!\n",
    "\n",
    "- **x‚ÇÄ**: Valid protein structure (backbone atom positions)\n",
    "- **x_t**: Progressively noisier structure\n",
    "- **x_T**: Complete random positions (no structure)\n",
    "\n",
    "### Key Differences for Proteins:\n",
    "\n",
    "1. **SE(3) Equivariance**: Network must respect rotations/translations\n",
    "2. **Constraints**: Bond lengths, angles must be reasonable\n",
    "3. **Conditioning**: Can condition on motifs, symmetry, binding partners\n",
    "\n",
    "### The Process:\n",
    "\n",
    "```\n",
    "Valid Protein ‚Üí Add Noise to Coordinates ‚Üí Pure Random Positions\n",
    "    (structured)                             (no structure)\n",
    "                      ‚Üì TRAINING ‚Üì\n",
    "Pure Random ‚Üí Neural Network Denoises ‚Üí Valid Protein Structure\n",
    "                (learns from real proteins)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a11fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate protein backbone diffusion (simplified 2D projection)\n",
    "# In reality, proteins are 3D, but we'll visualize in 2D for clarity\n",
    "\n",
    "def make_helix_2d(n_residues=20):\n",
    "    \"\"\"Generate a simple helix pattern in 2D (like looking down a helix).\"\"\"\n",
    "    t = np.linspace(0, 4*np.pi, n_residues)\n",
    "    r = np.linspace(0.5, 2, n_residues)  # Expanding radius\n",
    "    x = r * np.cos(t)\n",
    "    y = r * np.sin(t)\n",
    "    return np.stack([x, y], axis=1)\n",
    "\n",
    "# Create a simple \"protein\" (helix)\n",
    "protein_backbone = make_helix_2d(30)\n",
    "\n",
    "# Apply diffusion\n",
    "timesteps_protein = [0, 20, 50, 100]\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for idx, t in enumerate(timesteps_protein):\n",
    "    if t == 0:\n",
    "        X_t = protein_backbone\n",
    "        title = \"Original Structure\\n(t=0)\"\n",
    "    else:\n",
    "        X_t = forward_diffusion_2d(protein_backbone, t-1, betas)\n",
    "        title = f\"Diffused\\n(t={t})\"\n",
    "    \n",
    "    # Top row: scatter plot\n",
    "    axes[0, idx].plot(X_t[:, 0], X_t[:, 1], 'o-', markersize=6, linewidth=1.5, alpha=0.7)\n",
    "    axes[0, idx].set_title(title, fontsize=11, fontweight='bold')\n",
    "    axes[0, idx].set_xlim(-4, 4)\n",
    "    axes[0, idx].set_ylim(-4, 4)\n",
    "    axes[0, idx].set_aspect('equal')\n",
    "    axes[0, idx].grid(True, alpha=0.3)\n",
    "    axes[0, idx].set_xlabel('X coordinate (√Ö)', fontsize=9)\n",
    "    axes[0, idx].set_ylabel('Y coordinate (√Ö)', fontsize=9)\n",
    "    \n",
    "    # Bottom row: distance matrix (shows structure)\n",
    "    from scipy.spatial.distance import cdist\n",
    "    dist_matrix = cdist(X_t, X_t)\n",
    "    im = axes[1, idx].imshow(dist_matrix, cmap='viridis', aspect='auto')\n",
    "    axes[1, idx].set_title('Distance Matrix', fontsize=10)\n",
    "    axes[1, idx].set_xlabel('Residue', fontsize=9)\n",
    "    axes[1, idx].set_ylabel('Residue', fontsize=9)\n",
    "    if idx == 3:\n",
    "        plt.colorbar(im, ax=axes[1, idx], label='Distance (√Ö)')\n",
    "\n",
    "plt.suptitle('Protein Backbone Diffusion (2D Visualization)', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìå Key Observations:\")\n",
    "print(\"   - Top row: Backbone coordinates become random\")\n",
    "print(\"   - Bottom row: Distance matrix loses structure ‚Üí becomes uniform\")\n",
    "print(\"   - This is what RFDiffusion learns to reverse!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9148f7",
   "metadata": {},
   "source": [
    "## üìä Noise Schedules\n",
    "\n",
    "The choice of $\\beta_t$ (noise schedule) is important!\n",
    "\n",
    "### Common Schedules:\n",
    "\n",
    "1. **Linear**: $\\beta_t = \\beta_{\\min} + (\\beta_{\\max} - \\beta_{\\min}) \\frac{t}{T}$\n",
    "2. **Cosine**: Slower at the beginning, faster at the end\n",
    "3. **Quadratic**: Even more gradual\n",
    "\n",
    "Let's compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1038fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different noise schedules\n",
    "T = 100\n",
    "\n",
    "# Linear schedule\n",
    "betas_linear = np.linspace(0.0001, 0.02, T)\n",
    "\n",
    "# Cosine schedule\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = np.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = np.cos(((x / timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return np.clip(betas, 0, 0.999)\n",
    "\n",
    "betas_cosine = cosine_beta_schedule(T)\n",
    "\n",
    "# Quadratic schedule\n",
    "betas_quadratic = (np.linspace(0.0001**0.5, 0.02**0.5, T))**2\n",
    "\n",
    "# Plot schedules\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Beta values\n",
    "ax1.plot(betas_linear, label='Linear', linewidth=2)\n",
    "ax1.plot(betas_cosine, label='Cosine', linewidth=2)\n",
    "ax1.plot(betas_quadratic, label='Quadratic', linewidth=2)\n",
    "ax1.set_xlabel('Timestep', fontsize=12)\n",
    "ax1.set_ylabel('Œ≤ (noise level)', fontsize=12)\n",
    "ax1.set_title('Noise Schedules: Œ≤ Values', fontsize=13, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative product (signal retention)\n",
    "alphas_linear = 1 - betas_linear\n",
    "alphas_cosine = 1 - betas_cosine\n",
    "alphas_quadratic = 1 - betas_quadratic\n",
    "\n",
    "ax2.plot(np.cumprod(alphas_linear), label='Linear', linewidth=2)\n",
    "ax2.plot(np.cumprod(alphas_cosine), label='Cosine', linewidth=2)\n",
    "ax2.plot(np.cumprod(alphas_quadratic), label='Quadratic', linewidth=2)\n",
    "ax2.set_xlabel('Timestep', fontsize=12)\n",
    "ax2.set_ylabel('·æ± (signal retention)', fontsize=12)\n",
    "ax2.set_title('Signal Retention Over Time', fontsize=13, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìå Interpretation:\")\n",
    "print(\"   - Linear: Uniform noise addition\")\n",
    "print(\"   - Cosine: Preserves more signal early on\")\n",
    "print(\"   - Quadratic: Slower initial corruption\")\n",
    "print(\"\\nRFDiffusion uses a modified schedule optimized for proteins!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d07d6e",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "1. **Diffusion models** learn to gradually denoise data\n",
    "2. **Forward process** is fixed (add noise according to schedule)\n",
    "3. **Reverse process** is learned (neural network predicts noise to remove)\n",
    "4. **Training** is simple: predict the noise that was added\n",
    "5. **Sampling** starts from pure noise and iteratively denoises\n",
    "6. **For proteins**: Same idea, but with 3D coordinates and geometric constraints\n",
    "\n",
    "## ‚úÖ Self-Check Questions\n",
    "\n",
    "1. What are the two processes in a diffusion model?\n",
    "2. Why do we need a noise schedule?\n",
    "3. What does the neural network predict during training?\n",
    "4. How do we generate new samples?\n",
    "5. What's special about protein diffusion compared to image diffusion?\n",
    "\n",
    "## üí° Practice Exercise\n",
    "\n",
    "Try modifying the code above to:\n",
    "1. Use a different noise schedule\n",
    "2. Change the number of timesteps\n",
    "3. Apply diffusion to your own 2D dataset\n",
    "\n",
    "## üìñ Further Reading\n",
    "\n",
    "- [DDPM Paper](https://arxiv.org/abs/2006.11239) - Original denoising diffusion paper\n",
    "- [Score-Based Models](https://yang-song.net/blog/2021/score/) - Alternative perspective\n",
    "- [Diffusion Models Tutorial](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) - Lilian Weng's excellent blog\n",
    "\n",
    "## ‚è≠Ô∏è Next Notebook\n",
    "\n",
    "**03_protein_representation.ipynb** - Learn how proteins are encoded as input to RFDiffusion\n",
    "\n",
    "üí° **Still no GPU needed!** Next notebook covers data structures and representations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
