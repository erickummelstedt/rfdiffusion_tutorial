{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60586d37",
   "metadata": {},
   "source": [
    "# üé® Notebook 05: Unconditional Protein Generation\n",
    "\n",
    "**Learning Objective**: Generate novel protein backbones from scratch using diffusion models\n",
    "\n",
    "## üíª GPU Requirements\n",
    "\n",
    "**‚ö†Ô∏è GPU Optional but Recommended**\n",
    "- Works on CPU but generation will be slow (10-30 minutes per protein)\n",
    "- With GPU: 30 seconds - 2 minutes per protein\n",
    "- Recommended: T4 GPU or better (available free on Google Colab)\n",
    "\n",
    "**Running on Google Colab**:\n",
    "1. Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
    "2. See [colab_gpu_test.ipynb](../../colab_gpu_test.ipynb) to verify GPU is working\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "1. Complete RFDiffusion sampling loop\n",
    "2. GPU-accelerated protein generation\n",
    "3. Noise schedules and timestep selection\n",
    "4. Post-processing generated structures\n",
    "5. Visualization of generation process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed8b2a4",
   "metadata": {},
   "source": [
    "## üîß Setup and GPU Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.spatial.transform import Rotation\n",
    "import time\n",
    "\n",
    "# Detect and setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Running on CPU - generation will be slower\")\n",
    "    print(\"For faster generation, enable GPU in Runtime ‚Üí Change runtime type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7d54a6",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Build Simplified RFDiffusion Model\n",
    "\n",
    "We'll implement a simplified version that captures the key ideas:\n",
    "- SE(3) equivariant updates to rigid body frames\n",
    "- Diffusion process on backbone coordinates\n",
    "- GPU-accelerated inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a1d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedRFDiffusion(nn.Module):\n",
    "    \"\"\"Simplified RFDiffusion model for educational purposes.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=128, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embed timestep\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Process per-residue features\n",
    "        self.coord_embed = nn.Linear(3, hidden_dim)\n",
    "        \n",
    "        # Simple attention-based layers (simplified IPA)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim,\n",
    "                nhead=4,\n",
    "                dim_feedforward=hidden_dim*4,\n",
    "                batch_first=True\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Predict coordinate updates\n",
    "        self.coord_out = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, coords, t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            coords: (batch, n_residues, 3) - CŒ± coordinates\n",
    "            t: (batch,) - timestep (0 to 1, where 1 is pure noise)\n",
    "        \n",
    "        Returns:\n",
    "            coord_updates: (batch, n_residues, 3) - predicted denoising updates\n",
    "        \"\"\"\n",
    "        batch_size, n_res, _ = coords.shape\n",
    "        \n",
    "        # Embed timestep\n",
    "        t_embed = self.time_embed(t.view(-1, 1))  # (batch, hidden_dim)\n",
    "        t_embed = t_embed.unsqueeze(1).expand(-1, n_res, -1)  # (batch, n_res, hidden_dim)\n",
    "        \n",
    "        # Embed coordinates\n",
    "        coord_feat = self.coord_embed(coords)  # (batch, n_res, hidden_dim)\n",
    "        \n",
    "        # Combine features\n",
    "        x = coord_feat + t_embed\n",
    "        \n",
    "        # Apply transformer layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Predict coordinate updates\n",
    "        coord_updates = self.coord_out(x)\n",
    "        \n",
    "        return coord_updates\n",
    "\n",
    "# Initialize model and move to GPU\n",
    "model = SimplifiedRFDiffusion(hidden_dim=128, num_layers=4).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Model on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94532ebf",
   "metadata": {},
   "source": [
    "## üé≤ Diffusion Process Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"Cosine schedule as proposed in Improved DDPM.\"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "class DiffusionProcess:\n",
    "    \"\"\"Handles forward and reverse diffusion.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_timesteps=100, device='cpu'):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.device = device\n",
    "        \n",
    "        # Noise schedule\n",
    "        self.betas = cosine_beta_schedule(num_timesteps).to(device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "        print(f\"Diffusion schedule on device: {self.alphas_cumprod.device}\")\n",
    "    \n",
    "    def add_noise(self, x0, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward process: q(x_t | x_0)\n",
    "        \n",
    "        Args:\n",
    "            x0: (batch, n_res, 3) - clean coordinates\n",
    "            t: (batch,) - timestep indices (0 to num_timesteps-1)\n",
    "            noise: (batch, n_res, 3) - optional pre-generated noise\n",
    "        \n",
    "        Returns:\n",
    "            xt: noisy coordinates\n",
    "            noise: the noise that was added\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x0)\n",
    "        \n",
    "        # Get alpha values for timesteps\n",
    "        alpha_t = self.alphas_cumprod[t].view(-1, 1, 1)  # (batch, 1, 1)\n",
    "        \n",
    "        # q(x_t | x_0) = sqrt(alpha_t) * x_0 + sqrt(1 - alpha_t) * noise\n",
    "        xt = torch.sqrt(alpha_t) * x0 + torch.sqrt(1 - alpha_t) * noise\n",
    "        \n",
    "        return xt, noise\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def denoise_step(self, model, xt, t_idx):\n",
    "        \"\"\"\n",
    "        Single reverse diffusion step: p(x_{t-1} | x_t)\n",
    "        \n",
    "        Args:\n",
    "            model: denoising model\n",
    "            xt: (batch, n_res, 3) - noisy coordinates at timestep t\n",
    "            t_idx: integer timestep index\n",
    "        \n",
    "        Returns:\n",
    "            x_prev: coordinates at timestep t-1\n",
    "        \"\"\"\n",
    "        batch_size = xt.shape[0]\n",
    "        \n",
    "        # Timestep as continuous value (0 to 1)\n",
    "        t = torch.full((batch_size,), t_idx / self.num_timesteps, device=self.device)\n",
    "        \n",
    "        # Predict noise\n",
    "        predicted_noise = model(xt, t)\n",
    "        \n",
    "        # Compute x_0 prediction\n",
    "        alpha_t = self.alphas_cumprod[t_idx]\n",
    "        x0_pred = (xt - torch.sqrt(1 - alpha_t) * predicted_noise) / torch.sqrt(alpha_t)\n",
    "        \n",
    "        # Compute x_{t-1}\n",
    "        if t_idx > 0:\n",
    "            alpha_prev = self.alphas_cumprod[t_idx - 1]\n",
    "            beta_t = self.betas[t_idx]\n",
    "            \n",
    "            # Posterior mean\n",
    "            x_prev = torch.sqrt(alpha_prev) * x0_pred + \\\n",
    "                     torch.sqrt(1 - alpha_prev - beta_t) * predicted_noise\n",
    "            \n",
    "            # Add noise (except at last step)\n",
    "            noise = torch.randn_like(xt) * torch.sqrt(beta_t)\n",
    "            x_prev = x_prev + noise\n",
    "        else:\n",
    "            x_prev = x0_pred\n",
    "        \n",
    "        return x_prev\n",
    "\n",
    "# Initialize diffusion process\n",
    "diffusion = DiffusionProcess(num_timesteps=100, device=device)\n",
    "print(f\"‚úÖ Diffusion process ready with {diffusion.num_timesteps} timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41806d67",
   "metadata": {},
   "source": [
    "## üé® Generate Protein Backbone (GPU Accelerated!)\n",
    "\n",
    "Now let's generate a protein from pure noise. This will run on GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a0225",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_protein(model, diffusion, n_residues=50, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate a protein backbone from pure noise.\n",
    "    \n",
    "    Args:\n",
    "        model: trained (or untrained for demo) diffusion model\n",
    "        diffusion: DiffusionProcess instance\n",
    "        n_residues: number of residues to generate\n",
    "        device: 'cuda' or 'cpu'\n",
    "    \n",
    "    Returns:\n",
    "        coords: (n_residues, 3) final generated CŒ± coordinates\n",
    "        trajectory: list of intermediate coordinates for visualization\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Start from pure noise\n",
    "    xt = torch.randn(1, n_residues, 3, device=device) * 10.0  # Scale for protein-like distances\n",
    "    \n",
    "    trajectory = [xt[0].cpu().numpy()]\n",
    "    \n",
    "    print(f\"Generating {n_residues}-residue protein...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Reverse diffusion\n",
    "    for t in range(diffusion.num_timesteps - 1, -1, -1):\n",
    "        if t % 20 == 0:\n",
    "            print(f\"  Step {diffusion.num_timesteps - t}/{diffusion.num_timesteps}\")\n",
    "        \n",
    "        xt = diffusion.denoise_step(model, xt, t)\n",
    "        \n",
    "        # Save trajectory snapshots\n",
    "        if t % 10 == 0:\n",
    "            trajectory.append(xt[0].cpu().numpy())\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"‚úÖ Generation complete in {elapsed:.2f}s\")\n",
    "    print(f\"   ({elapsed/n_residues:.3f}s per residue)\")\n",
    "    \n",
    "    coords = xt[0].cpu().numpy()\n",
    "    \n",
    "    return coords, trajectory\n",
    "\n",
    "# Generate a protein!\n",
    "n_residues = 30  # Start small for demo\n",
    "coords_generated, trajectory = generate_protein(model, diffusion, n_residues, device=device)\n",
    "\n",
    "print(f\"\\nGenerated coordinates shape: {coords_generated.shape}\")\n",
    "print(f\"Coordinate range: [{coords_generated.min():.2f}, {coords_generated.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f04d1",
   "metadata": {},
   "source": [
    "## üìä Visualize Generated Protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_generation_process(trajectory):\n",
    "    \"\"\"Show how the protein emerges from noise.\"\"\"\n",
    "    n_snapshots = min(len(trajectory), 6)\n",
    "    indices = np.linspace(0, len(trajectory)-1, n_snapshots, dtype=int)\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 3))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        coords = trajectory[idx]\n",
    "        ax = fig.add_subplot(1, n_snapshots, i+1, projection='3d')\n",
    "        \n",
    "        ax.plot(coords[:, 0], coords[:, 1], coords[:, 2], \n",
    "                'o-', linewidth=2, markersize=6, alpha=0.7)\n",
    "        \n",
    "        step = (len(trajectory) - 1 - idx) * 10\n",
    "        ax.set_title(f'Step {step}/{diffusion.num_timesteps}', fontweight='bold')\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_zlabel('Z')\n",
    "        \n",
    "        # Set consistent scale\n",
    "        ax.set_xlim(-15, 15)\n",
    "        ax.set_ylim(-15, 15)\n",
    "        ax.set_zlim(-15, 15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the generation process\n",
    "visualize_generation_process(trajectory)\n",
    "\n",
    "# Final structure\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(coords_generated[:, 0], coords_generated[:, 1], coords_generated[:, 2],\n",
    "        'o-', linewidth=3, markersize=8, color='#2E86AB', alpha=0.8)\n",
    "\n",
    "# Color by position (N-terminus to C-terminus)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(coords_generated)))\n",
    "ax.scatter(coords_generated[:, 0], coords_generated[:, 1], coords_generated[:, 2],\n",
    "           c=colors, s=100, alpha=0.9, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel('X (√Ö)', fontsize=12)\n",
    "ax.set_ylabel('Y (√Ö)', fontsize=12)\n",
    "ax.set_zlabel('Z (√Ö)', fontsize=12)\n",
    "ax.set_title(f'Generated Protein ({n_residues} residues)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Analyze distances\n",
    "distances = np.linalg.norm(np.diff(coords_generated, axis=0), axis=1)\n",
    "print(f\"\\nCŒ±-CŒ± distances:\")\n",
    "print(f\"  Mean: {distances.mean():.2f} √Ö (expected ~3.8 √Ö)\")\n",
    "print(f\"  Std:  {distances.std():.2f} √Ö\")\n",
    "print(f\"  Min:  {distances.min():.2f} √Ö\")\n",
    "print(f\"  Max:  {distances.max():.2f} √Ö\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a6ba9f",
   "metadata": {},
   "source": [
    "## üîë Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **GPU Acceleration** ‚ö°\n",
    "   - Model and data automatically move to GPU with `.to(device)`\n",
    "   - Generation is 10-100x faster on GPU\n",
    "   - All tensor operations happen on the same device\n",
    "\n",
    "2. **Diffusion Sampling Loop**\n",
    "   - Start from pure random noise\n",
    "   - Iteratively denoise using the model\n",
    "   - Each step predicts and removes a bit of noise\n",
    "\n",
    "3. **Untrained Model Limitations**\n",
    "   - This model wasn't trained, so output is still somewhat random\n",
    "   - Real RFDiffusion is trained on thousands of protein structures\n",
    "   - Training teaches realistic protein geometry and secondary structures\n",
    "\n",
    "4. **Next Steps for Real Generation**\n",
    "   - Train model on PDB structures\n",
    "   - Add proper SE(3) equivariant layers (full IPA)\n",
    "   - Include side-chain prediction\n",
    "   - Add structural validity checks\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Practice Exercises\n",
    "\n",
    "1. **Experiment with generation parameters**:\n",
    "   - Try different numbers of residues (20, 50, 100)\n",
    "   - Modify noise schedules\n",
    "   - Change number of denoising steps\n",
    "\n",
    "2. **Benchmark GPU speedup**:\n",
    "   - Time generation on CPU vs GPU\n",
    "   - Try different batch sizes\n",
    "   - Profile memory usage\n",
    "\n",
    "3. **Improve the model**:\n",
    "   - Add more layers\n",
    "   - Increase hidden dimensions\n",
    "   - Implement proper attention mechanisms\n",
    "\n",
    "---\n",
    "\n",
    "## ‚û°Ô∏è Next Notebook\n",
    "\n",
    "**Notebook 06: Motif Scaffolding** - Design proteins around specific functional motifs by conditioning the generation process.\n",
    "\n",
    "**Note**: To train this model properly, you would need:\n",
    "- PDB protein structure dataset\n",
    "- Training loop with loss computation\n",
    "- Several hours of GPU time\n",
    "- See advanced tutorials for full training pipeline"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
